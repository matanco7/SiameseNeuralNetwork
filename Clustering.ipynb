{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np \n",
    "from sklearn.cluster import KMeans\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import random\n",
    "import matplotlib as mpl\n",
    "font = {'family' : 'Calibri',\n",
    "        'weight' : 'light',\n",
    "        'size'   : 12}\n",
    "\n",
    "mpl.rc('font', **font)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\MATANC~1\\AppData\\Local\\Temp/ipykernel_20624/2572618638.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mall_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_scan\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0m_scan\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: ''"
     ]
    }
   ],
   "source": [
    "results = os.listdir(data_path)\n",
    "all_df = []\n",
    "\n",
    "for _scan in results:\n",
    "    df = pd.read_csv(os.path.join(data_path,_scan))\n",
    "    df = df.dropna()\n",
    "    df['UniqueIdentifier'] = _scan[:-4]\n",
    "    df = df.rename(columns={'Unnamed: 0':'frame'})\n",
    "    \n",
    "    all_df.append(df)\n",
    "\n",
    "all_df = pd.concat(all_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_df = all_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = df.drop(['frame','UniqueIdentifier'],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = data_df.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "distortions = []\n",
    "inertias = []\n",
    "mapping1 = {}\n",
    "mapping2 = {}\n",
    "K = range(1, 30)\n",
    "  \n",
    "for k in K:\n",
    "    # Building and fitting the model\n",
    "    kmeanModel = KMeans(n_clusters=k).fit(vectors)\n",
    "    kmeanModel.fit(vectors)\n",
    "  \n",
    "    distortions.append(sum(np.min(cdist(vectors, kmeanModel.cluster_centers_,\n",
    "                                        'euclidean'), axis=1)) / vectors.shape[0])\n",
    "    inertias.append(kmeanModel.inertia_)\n",
    "  \n",
    "    mapping1[k] = sum(np.min(cdist(vectors, kmeanModel.cluster_centers_,\n",
    "                                   'euclidean'), axis=1)) / vectors.shape[0]\n",
    "    mapping2[k] = kmeanModel.inertia_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('Values of K')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('Elbow Method for output size of 128')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View mapping to clusters on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = r''\n",
    "image_path = r''\n",
    "\n",
    "npy_files = os.listdir(image_path)\n",
    "random.shuffle(npy_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4, random_state=0)\n",
    "kmeans.fit(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for npy_file in npy_files:\n",
    "    print(npy_file)\n",
    "    im = np.load(os.path.join(image_path,npy_file))\n",
    "    number_idx = npy_file.rfind('_')\n",
    "    scan = npy_file[:number_idx]\n",
    "    frame = int(npy_file[number_idx+1:-4])\n",
    "\n",
    "    vector = all_df[(all_df.frame==frame)&(all_df.UniqueIdentifier==scan)].drop(['frame','UniqueIdentifier'],axis=1).to_numpy()#.astype('float32')\n",
    "    if len(vector)>0:\n",
    "        cluster = kmeans.predict(vector)[0]\n",
    "        cluster_save_path = save_path + '\\\\' + str(cluster)\n",
    "\n",
    "        if not os.path.isdir(cluster_save_path):\n",
    "            os.mkdir(cluster_save_path)\n",
    "\n",
    "        plt.imshow(im,cmap = 'gray')\n",
    "        plt.axis('off')\n",
    "        plt.savefig(os.path.join(cluster_save_path,'{}_{}.png'.format(scan,frame)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate clustering on a labeled test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Model_path = r'C:\\Users\\ShelleyJoyLevy\\Documents\\similarity\\09_27_22_deep_stride_64'\n",
    "os.chdir(Model_path)\n",
    "from SimilarityModelDeepStride64 import SimilarityNetwork,TripletLoss\n",
    "\n",
    "\n",
    "net = SimilarityNetwork()\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "net.load_state_dict(torch.load(Model_path + '\\BestDev.p',map_location=torch.device(device)))\n",
    "net.eval();\n",
    "data_path = r'\\\\nv-nas01\\Algorithm\\python\\codes\\semantic_segmentation\\Data\\Hoct_label_manual\\labeld\\final\\Asclepix_AMD\\Labeld_Data'\n",
    "mats = os.listdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = []\n",
    "save_path = r''\n",
    "\n",
    "for matname in mats:\n",
    "    curr_df = {}\n",
    "    mat = sio.loadmat(os.path.join(data_path,matname))\n",
    "    im = mat['I'].astype('float32')\n",
    "    curr_df['with_SRF'] = 1 in np.unique(mat['Ilab'])\n",
    "    curr_df['with_IRF'] = 2 in np.unique(mat['Ilab'])\n",
    "    curr_df['wet'] = (curr_df['with_SRF']) or (curr_df['with_IRF']) #or (curr_df['with_ERM'])\n",
    "    curr_df['dry'] = not curr_df['wet']\n",
    "\n",
    "    curr_df['fname'] = matname[:-4]\n",
    "    I_tensor = torch.from_numpy(im)\n",
    "    I_predict = torch.from_numpy(np.expand_dims(I_tensor, (0,1)))\n",
    "    score = np.squeeze(net(I_predict).detach().numpy()).reshape(1, -1).astype('float64')\n",
    "    cluster = kmeans.predict(score)[0]\n",
    "    curr_df['cluster'] = cluster\n",
    "    result_df.append(curr_df)\n",
    "    print(matname,cluster,curr_df['with_SRF'],curr_df['with_IRF'])\n",
    "    cluster_save_path = save_path + '\\\\' + str(cluster)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.groupby('cluster').sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "6f8d759ffdb570e708ea53cf774c8e93bb3d8b5f621b0bdc24754b0cc8bff713"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
